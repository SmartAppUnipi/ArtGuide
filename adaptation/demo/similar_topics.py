# -*- coding: utf-8 -*-
"""Similar Topics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NMJTjbFfJYCeXDV7IsPXkP6y_K8Wj1nv

# Init
"""

!pip install googledrivedownloader
!unzip -n dataset.zip

"""Downalod and convert pretrained Stanford GloVe:"""

! wget -c http://nlp.stanford.edu/data/glove.6B.zip
! unzip -n glove.6B.zip

from gensim.scripts.glove2word2vec import glove2word2vec

glove_input_file = 'glove.6B.100d.txt'
word2vec_output_file = 'glove.6B.100d.txt.word2vec'
glove2word2vec(glove_input_file, word2vec_output_file)

"""Downalod and convert pretrained Google News Word2Vec:"""

from google_drive_downloader import GoogleDriveDownloader as gdd
#https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit
gdd.download_file_from_google_drive(file_id='0B7XkCwpI5KDYNlNUTTlSS21pQmM',
                                    dest_path='./GoogleNews-vectors-negative300.bin.gz',
                                    unzip=False)

!gunzip GoogleNews-vectors-negative300.bin.gz

"""Set **environment variables**:"""

# Commented out IPython magic to ensure Python compatibility.
# %pylab inline
import pandas as pd
from dataloader import DataLoader
import spacy
from gensim.models import KeyedVectors


WORDS_IN_TOPIC = 20
NUM_TOPICS = 6+1
TOPIC_WORDS = ['history', 'chemistry', 'biography', 'fun facts', 'art movements', 'techniques']
TARGET_TOPIC = 'chemistry'

dataloader = DataLoader(data_dir='/content/dataset', topics=TOPIC_WORDS)
df = pd.DataFrame(list(dataloader))


stanford_glove = KeyedVectors.load_word2vec_format('glove.6B.100d.txt.word2vec', binary=False)
print("Loaded Stanford GloVe!")
googlenews_word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
print("Loaded Google News Word2Vec!")

df.columns

"""# BOW"""

nlp = spacy.blank("en")
texts = df[0]

en = spacy.load('en')
doc_en = en(texts[0])
features = [[t.orth_, t.lemma_, t.pos_, t.tag_] for t  in doc_en[20:]]
print([(ent.text, ent.label_) for ent in doc_en.ents])

# Commented out IPython magic to ensure Python compatibility.
# %time spacy_docs = list(nlp.pipe(texts))

docs = [[t.lemma_.lower() for t in doc if len(t.orth_) > 3 and not t.is_stop] for doc in spacy_docs]
print(docs[:3])

import re
from gensim.models.phrases import Phrases

bigram = Phrases(docs, min_count=10)

for idx in range(len(docs)):
    for token in bigram[docs[idx]]:
        if '_' in token:  # bigrams can be recognized by the "_" that joins the invidual words
            docs[idx].append(token)

docs[1]

from gensim.corpora import Dictionary

dictionary = Dictionary(docs)
print('Number of unique words in original documents:', len(dictionary))

dictionary.filter_extremes(no_below=1, no_above=0.8)
print('Number of unique words after removing rare and common words:', len(dictionary))

print("Example representation of document 3:", dictionary.doc2bow(docs[1]))

import seaborn as sns
sns.set()  # defines the style of the plots to be seaborn style

# I made a function out of this since I will use it again later on 
def word_frequency_barplot(df, nr_top_words=50):
    """ df should have a column named count.
    """
    fig, ax = plt.subplots(1,1,figsize=(20,5))

    sns.barplot(list(range(nr_top_words)), df['count'].values[:nr_top_words], palette='hls', ax=ax)

    ax.set_xticks(list(range(nr_top_words)))
    ax.set_xticklabels(df.index[:nr_top_words], fontsize=14, rotation=90)
    return ax

cleansed_words_df = pd.DataFrame.from_dict(dictionary.token2id, orient='index')
cleansed_words_df.rename(columns={0: 'id'}, inplace=True)

cleansed_words_df['count'] = list(map(lambda id_: dictionary.dfs.get(id_), cleansed_words_df.id))
del cleansed_words_df['id']

cleansed_words_df.sort_values('count', ascending=False, inplace=True)

ax = word_frequency_barplot(cleansed_words_df)
ax.set_title("Document Frequencies (Number of documents a word appears in)", fontsize=16);

"""# LDA on BOW"""

corpus = [dictionary.doc2bow(doc) for doc in docs]

print(docs[0])
print(corpus[0])

# Commented out IPython magic to ensure Python compatibility.
from gensim.models.ldamulticore import LdaMulticore

# %time lda = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=NUM_TOPICS, chunksize=1000, passes=10, random_state=1)

# words rappresenting topic 0
print([word for (word, prob) in lda.show_topic(0, topn=WORDS_IN_TOPIC)])

for (topic, words) in lda.print_topics():
    print(topic+1, ":", words)

dfres = pd.DataFrame()
results = []
for index, data in enumerate(zip(texts, docs)):
    text, doc = data
    result = [] 
    result += dataloader.info(index)
    result += [(topic+1, prob) for (topic, prob) in lda[dictionary.doc2bow(doc)] if prob > 0.1]
    result.append(text)
    results.append(result)

print("With LDA we can find the main topics with a discrete precision (but we don't know exactly the subject of the topic):")
pd.DataFrame(results, columns=['Subject','Ground truth','Filename','Topics,Probability','Text'])

"""# Embeddings Word2Vec"""

import gensim

salient_words_per_text = []

for doc in docs:
  topic_ids = [topic for (topic, prob) in lda[dictionary.doc2bow(doc)] if prob > 0.1]
  words = list(map(lambda x: [words for (words, prob) in x]+TOPIC_WORDS, [lda.show_topic(topic_id, topn=WORDS_IN_TOPIC) for topic_id in topic_ids]))
  salient_words_per_text += words

print(f"Salient words topics for each text:")
print(salient_words_per_text)

print(f"\nWord embedding for {TARGET_TOPIC}:")
print(googlenews_word2vec[TARGET_TOPIC])

googlenews_word2vec.wv.similar_by_word(TARGET_TOPIC, topn=10)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import pandas as pd
import matplotlib
import matplotlib.pyplot as plt

from sklearn.manifold import TSNE

selected_words = [w[0] for w in googlenews_word2vec.wv.most_similar(positive=[TARGET_TOPIC], topn=200)]
embeddings = [googlenews_word2vec[w] for w in selected_words if w in googlenews_word2vec]

mapped_embeddings = TSNE(n_components=2, metric='cosine', init='pca').fit_transform(embeddings)

plt.figure(figsize=(10,10))
x = mapped_embeddings[:,0]
y = mapped_embeddings[:,1]
plt.scatter(x, y)

for i, txt in enumerate(selected_words):
    plt.annotate(txt, (x[i], y[i]))
plt.show()

"""# Clustering"""

#----------------------------------------------------------------------
# Visualize the clustering
def plot_clustering(X_red, y, labels, title=None):
    x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)
    X_red = (X_red - x_min) / (x_max - x_min)

    plt.figure(figsize=(10, 10))
    for i in range(X_red.shape[0]):
        plt.text(X_red[i, 0], X_red[i, 1], str(y[i]),
                 color=plt.cm.nipy_spectral(labels[i] / 10.),
                 fontdict={'weight': 'bold', 'size': 9})

    #plt.xticks([])
    #plt.yticks([])
    if title is not None:
        plt.title(title, size=17)
    #plt.axis('off')
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])

from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import normalize

salient_words = salient_words_per_text[0]
vocab = salient_words
vectors = [googlenews_word2vec[w] for w in vocab if w in googlenews_word2vec]
vectors_norm = normalize(vectors)

clusterer = AgglomerativeClustering(n_clusters=NUM_TOPICS, affinity="euclidean")
clusters = clusterer.fit_predict(vectors_norm)

cluster_dictionary = {}
for cluster, word in zip(clusters, vocab): 
    if cluster not in cluster_dictionary:
        cluster_dictionary[cluster] = []
    cluster_dictionary[cluster].append(word)

for x in cluster_dictionary:
    if "history" in cluster_dictionary[x]:
        print(cluster_dictionary[x])

plot_clustering(vectors_norm, vocab, clusterer.labels_)

"""# Test
Testing the procedure searching the clusters on Word2Vec an GloVe LDA's topic words
"""

class DocumentModel():
  def __init__(self, text, info=[]):
    self.embeddings = []
    self.info = info
    self.text = text 
    self.doc = []
    self.salient_words = []
    self.assigned_topics = []
    self.clusters = []

models = []
for i in range(len(dataloader)):
  model = DocumentModel(dataloader.__item__(i), dataloader.info(i))
  model.doc = docs[i]
  models.append(model)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import gensim
# 
# TARGET_TOPIC = 'chemistry'
# 
# for model in models:
#   doc = model.doc
#   topic_ids = [topic for (topic, prob) in lda[dictionary.doc2bow(doc)] if prob > 0.1]
#   words = list(map(lambda x: [words for (words, prob) in x]+TOPIC_WORDS, [lda.show_topic(topic_id, topn=WORDS_IN_TOPIC) for topic_id in topic_ids]))
#   model.salient_words = words[0]
# 
# print(f"Salient words topics for each text:")
# for i, model in enumerate(models): print(f"Text {i+1} salient words: {model.salient_words}")
# 
# for model in models:
#   # Load GoogleNews Word2Vec and Stanford GloVe 
#   embeddings_models = [googlenews_word2vec, stanford_glove]
#   vocab = model.salient_words
#   for embeddings_model in embeddings_models:
#     model.embeddings.append([embeddings_model[w] for w in vocab if w in embeddings_model])
# 
# print(f"Different embeddings per document: {len(models[0].embeddings)}")

# One w2v model foreach document
for model in models: model.clusters = []
for model in models:
  for embedding in model.embeddings:
    vectors_norm = normalize(embedding)

    clusterer = AgglomerativeClustering(n_clusters=NUM_TOPICS, affinity="euclidean")
    clusters = clusterer.fit_predict(vectors_norm)

    cluster_dictionary = {}
    # translate clutesrs of world_id in clusters of words
    for cluster, word in zip(clusters, vocab): 
        if cluster not in cluster_dictionary:
            cluster_dictionary[cluster] = []
        cluster_dictionary[cluster].append(word)

    # calculate words in cluster / words in topic
    topic_clusters_len = {}
    total_words_in_cluster = 0
    for x in cluster_dictionary:
      for topic in TOPIC_WORDS:
        if topic in cluster_dictionary[x]:
          topic_clusters_len[topic] = len(cluster_dictionary[x])
      total_words_in_cluster += len(cluster_dictionary[x])

    main_clusters = ([(x,topic_clusters_len[x]/total_words_in_cluster) for x in topic_clusters_len])

    for topic in main_clusters:
      find_indx = [i for i,x in enumerate(model.clusters) if x[0]==topic[0]]
      if len(find_indx)>0:
        # if topic is yet in model.clusters add only probability
        model.clusters[find_indx[0]] = (topic[0], model.clusters[find_indx[0]][1]+topic[1])
      else:
        # else add topic to model.clusters
        model.clusters += [topic]

[print(model.clusters) for model in models]

texts_affinity = []
for model in models:
  texts_affinity += [x[1] for x in model.clusters if x[0]==TARGET_TOPIC]

for i in range(len(models)):
  print(f"{models[i].info} -> affinity:", texts_affinity[i])

best_text = np.argmax(texts_affinity)

print(f"Selected text for topic `{TARGET_TOPIC}` is: {models[best_text].info}")